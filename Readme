
 roboweld

 21 December 2020.

 I just aware that if the keyence driver node is not stopped by a controll C it will carry on
 building up the point cloud. That can become very large. 

 There should be a way to detect if the Laser scanner is switched on and off. On second thought
 this should be done by the program itself and do not need to find out from the scanner.

 Now I have the problem is because the robot and the scanner are not controlled by my program,
 roboweld, but by the Teach Pandent program. So, for the time being, leave it and just switch
 on and off the Keyence Driver Node by hand.

 One other observation, the profile given by the scanner will not be perfect if both the USB
 and the Ethernet are connected. The software in Windows will behave better when the TCP/IP 
 program is stopped. At the same time, if the the Driver Note is scanning, unplug the USB will
 give a better profile.

 Start to workout (1) the cross section area. That is the area given by the 1 line profile.
 (2) the cumulative volumn. That is the cross section area multiplied by the distance travelled.
 (3) Show both of them interactively in RViz.

 19 December 2020.

 Now, the profiles from the Keyence Laser Scanner are published in the world frame in one big
 point cloud.

 At the same time, because the Realsense D435i camera has already been installed and software
 is also working, just launch the driver will produce the point cloud:

   roslaunch realsense2_camera rs_camera.launch camera:=d435i enable_pointcloud:=true

 This will show the point cloud in RViz.

 17 December 2020.

 The Keyence Laser Scanner has been set to the ip address of 192.168.0.5 using the LJ Navigator
 in Windows. Also there the sampling rate was set to 20 Hz.

 This morning, I also connected the REMOTE of the scanner to the I/O controll relay from the
 UR5 controll box. This will only switch on the Laser under the control of the program.

 The next step is to put all the one line point clouds into a big point cloud. Each of the 
 one line point cloud has its reference frame as the optiical frame of the scanner. Before all
 these one line point clouds be concatinated into one big one, each of them must be transformed
 to the world coordinate to be consistant.

 Try to work out how this can be done.


 16 December 2020.

 It seems that I have already installed libsocket successfully for the Keyence Laser Scanner
 LJ7200. The next step will be use it to scan a groove and produce a profile in Point Cloud.

 1. Start the setupKeyence.launch - this will start the RViz and show the UR5 and let us to 
    Plan and Execute a pose.
 2. If the Kinect V2 can identify a groove then it can drive the torch or the Laser Scanner to 
    follow the groove
 3. Switch on the Laser as if switching on the Welding Torch.
 4. At the same time, switch on the RealSense camera to catch another Point Cloud of the same
    groove.
 5. Then we can compare the two, or even three Point Clouds captured by the three different
    sensors.

 Note: Do not worry about the Descartes for the time being and just use URx to drive the end
       effector.

 Create a new (ROS) node in roboweld_core and call it scan_node.

 Okay, I miss understood the whole thing! It is not necessary to write a separate node to get
 a PointCloud from the Keyence Scanner. According to the Readme file, just do a rosrun of the
 driver node with the _controller_ip will do. It will publish the PointCloud in the 'profiles'
 topic. Note that nothing will be published if nothing subscribes the topic.

 15 December 2020.

 Put the Keyence LJ7200 Laser Scanner on the Straight Fronius Torch holder together with
 the torch holder.

 Modified the roboweldKeyence.xacro file so that (1) when use the RViz and Planning and 
 Execution the Interactive Marker moves correctly, (2) the UR5 moves in front of the table
 instead of at the back of the table.

 28 November 2020.

 Cloned from Github, use VS Code at home and push back to Github.
 Okay, tested and it seems to be working.

 Needs to install freenect2 locally.

 27 November 2020.

 Changed from experiment back to roboweld.
 Start to use git and github in vs code.
 
 experiment

 28 October 2020.

 Copied the perception_node.cpp from 23 Oct 2019 / nrc-welding_ws.

 Before doing anything, to start the Kinect V2 first by

 $ roslaunch kinect2_bridge kinect2_bridge.launch

 Because the URx that I am going to use has the coordinate different from the one that I used
 therefore, as it is, when cropped some of the table is cropped.

 Not only this but because the X and Y axes exchanged, the algorithm to find the way points
 does not work. So it is necessary to change the X and Y in that algorithm as well!

 27 October 2020.

 It has been deceided to leave the Descartes to do trajectory planning for the time being.

 Instead URx will be used. This will incorporate the URx into this package.

 A new folder was placed under src/roboweld_core/ in parallel with src, script. urx is placed
 in that folder and a new file motion_URx.py is created.
 
 The idea is to go back to more than a year ago, use the Perception node to find the welding
 groove. Since it is going to use the URx for motion, it is not going to use Descartes anymore.

 12 October 2020.

 It would be better to plan to the path directly instead of referring to the center of the
 object.

 Using IK Solvers Other Than KDL With Calibration Data #226

 rosrun xacro xacro -o calibrated.urdf ur5_robot.urdf.xacro kinematics_config:=/<YOUR_PATH>/calibration.yaml

 5 October 2020.

 The end effector is not going to the center of the workpiece. It will start from the beginning
 of the trajectory.

 Now the default joint velocity was set to 0.03 radian per second.

 The EEF should find the path first, then move to the first point before starting the trajectory.

 9 September 2020.

 The moveit_setup_assistant was not working, did not display the robot after loading the model.
 Cloned the melodic-devel into the roboweld workspace and recompile with catkin_make, it is now
 working.

 It CAN drive the new UR5. Do not always do this. Test with the RViz simulation whenever
 possible. Change the ros_controllers.yaml file in the config folder under
 roboweld_moveit_config to use 
 action_ns: joint_trajectory_action
 for simulation.

 It has the RealSense D435i Camera sideway under the torch holder now. Take that away.
 Actually needs to rearrange according to the old one used in the lab.
 (1) Change the ur5.urdf.xaxro in the /fmauch_universal_robot/ur_description/urdf:
    (a) Comment out the wrist_3_passive_joint
    (b) Comment out the link tool0 at the end of the file
 (2) Change the roboweldfron1d435i.xacro in the /roboweld_support/urdf
    (a) 

 8 September 2020.

 This folder uses the bent torch and drives the real UR5.

 use the command:

 roslaunch roboweld_moveit_config roboweld_planning_execution sim:=false robot_ip:=192.168.0.103

 to drive the real UR5.

 To change it back to simulate in RViz, make sure the ros_controllers:yaml file in the 
 roboweld_moveit_config/config folder change the action_ns: to

 scaled_pos_traj_controller/follow_joint_trajectory


